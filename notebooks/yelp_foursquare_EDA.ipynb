{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports using python 3.9.20\n",
    "import pandas as pd\n",
    "import os # for our API keys\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in our bike dataframe\n",
    "bike_df = pd.read_pickle('../data/bike_dataframe.pkl')\n",
    "\n",
    "# load in our foursquare and Yelp API keys\n",
    "FOURSQUARE_KEY = os.getenv('FOURSQUARE_API_KEY')\n",
    "YELP_KEY = os.getenv('YELP_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foursquare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a request to Foursquare with a small radius (1000m) for all the bike stations in your city of choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial code built with the help of AI however it was heavily butchered to the point the only thing remaining is the description\n",
    "def get_venues_fs(latitude, longitude, radius, api_key, categories): \n",
    "    \"\"\"\n",
    "    Get venues from foursquare with a specified place type and coordinates.\n",
    "    Args:\n",
    "        latitude (float): latitude for query (must be combined with longitude)\n",
    "        longitude (float): longitude for query (must be combined with latitude)\n",
    "        api_key (str): foursquare API to use for query\n",
    "        categories (str) : Foursquare-recognized place type. If not passed no place_type will be specified. Separate ids with commas\n",
    "    \n",
    "    Returns:\n",
    "        response: response object from the requests library.\n",
    "    \"\"\"  \n",
    "    url = 'https://api.foursquare.com/v3/places/search?ll=' + str(latitude) + \",\" + str(longitude) + \"&radius=\" + str(radius) + \"&limit=50\"#+ \"&categories=\" + categories\n",
    "\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Authorization\" : api_key,\n",
    "    }\n",
    "    response = requests.get(url=url,headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse through the response to get the POI (such as restaurants, bars, etc) details you want (ratings, name, location, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set our base variables and create the list we will be populating with all of the information from the API's\n",
    "categories=\"None-Set\"\n",
    "\n",
    "#dropping to radius of 500 as I found maxed out my call for too many results\n",
    "radius = 500\n",
    "responses = []\n",
    "\n",
    "# Run through the bike data frame and get the longitude and latitude for every row, put it into foursquare to get our responses\n",
    "for index, row in bike_df.iterrows():\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "    res = get_venues_fs(latitude, longitude, radius, FOURSQUARE_KEY, categories)\n",
    "    \n",
    "    # make sure we got a valid response\n",
    "    if res.status_code == 200:\n",
    "        responses.append(res.json())\n",
    "    else:\n",
    "        print(\"fail at: \", index, res.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your parsed results into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe for foursquare (fsq)\n",
    "fsq_df = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "#loop through all of the responses, convert them 1 by 1 into a dataframe and concatenate them all together\n",
    "while i < len(responses):\n",
    "    try:\n",
    "        temp_df = pd.DataFrame(responses[i]['results'])\n",
    "        temp_df['bs_id'] = bike_df.at[i, 'bs_id']\n",
    "        i += 1\n",
    "        fsq_df = pd.concat([fsq_df, temp_df], ignore_index = True)\n",
    "    except:\n",
    "        print(\"fail at: \", i)\n",
    "        i+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out all the different columns that have multiple layers to them\n",
    "categories_series = fsq_df['categories']\n",
    "geocodes_series = fsq_df['geocodes'] \n",
    "location_series = fsq_df['location']\n",
    "\n",
    "# related places isn't relevant to our research so we will be leaving that out.\n",
    "related_places_series = fsq_df['related_places']\n",
    "\n",
    "# drop the above columns for cleaning\n",
    "fsq_df.drop(['related_places', 'location', 'geocodes','categories'], axis = 1, inplace = True)\n",
    "\n",
    "#add the geocode information back in, once we have the full df we will compare the three columns and see if they are required to be kept\n",
    "fsq_df['main_latitude'] = [d['main']['latitude'] if 'main' in d else None for d in geocodes_series]\n",
    "fsq_df['main_longitude'] = [d['main']['longitude'] if 'main' in d else None for d in geocodes_series]\n",
    "fsq_df['drop_off_latitude'] = [d['drop_off']['latitude'] if 'drop_off' in d else None for d in geocodes_series]\n",
    "fsq_df['drop_off_longitude'] = [d['drop_off']['longitude'] if 'drop_off' in d else None for d in geocodes_series]\n",
    "fsq_df['roof_latitude'] = [d['roof']['latitude'] if 'roof' in d else None for d in geocodes_series]\n",
    "fsq_df['roof_longitude'] = [d['roof']['longitude'] if 'roof' in d else None for d in geocodes_series]\n",
    "\n",
    "# break the location down and add it back in\n",
    "location_df = location_series.apply(pd.Series)\n",
    "fsq_df = pd.merge(fsq_df, location_df, left_index=True, right_index=True)\n",
    "categories_list = []\n",
    "\n",
    "#rename the distance column to fsq_distance\n",
    "fsq_df.rename(columns={'distance': 'fsq_distance'})\n",
    "\n",
    "#add sets of all the categories into the dataframe\n",
    "def extract_ids(data): #quick function to go through the json and return me all of the sets of the id's (AI assisted for building this function)\n",
    "  ids = [item['id'] for item in data]\n",
    "  return set(ids)\n",
    "\n",
    "for item in categories_series:\n",
    "  id_set = extract_ids(item)\n",
    "  categories_list.append(id_set)\n",
    "fsq_df['categories_ids'] = categories_list\n",
    "\n",
    "# seperate the categories into a seperate dataframe we can save as its own table\n",
    "fsq_categories_df = fsq_df[['fsq_id', 'categories_ids']].explode('categories_ids')\n",
    "\n",
    "# drop unnecessary columns that are just going to slow us down for our current project.\n",
    "fsq_df.drop(['chains',\n",
    "              'link',\n",
    "                'census_block',\n",
    "                  'drop_off_latitude',\n",
    "                    'drop_off_longitude',\n",
    "                      'roof_latitude',\n",
    "                        'roof_longitude',\n",
    "                          'formatted_address',\n",
    "                            'cross_street',\n",
    "                              'closed_bucket',\n",
    "                                'timezone',\n",
    "                                  'country',\n",
    "                                    'dma',\n",
    "                                      'locality',\n",
    "                                        'region',\n",
    "                                          'address_extended',\n",
    "                                            'po_box',\n",
    "                                              'categories_ids'],\n",
    "                                                axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe to a pickle\n",
    "fsq_df.to_pickle('../data/fsq_dataframe.pkl')\n",
    "fsq_categories_df.to_pickle('../data/fsq_categories_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a request to Yelp with a small radius (1000m) for all the bike stations in your city of choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial code built with the help of AI however it was heavily butchered to the point the only thing remaining is the description\n",
    "def get_venues_yelp(latitude, longitude, radius, api_key, categories): \n",
    "    \"\"\"\n",
    "    Get venues from YELP with a specified place type and coordinates.\n",
    "    Args:\n",
    "        latitude (float): latitude for query (must be combined with longitude)\n",
    "        longitude (float): longitude for query (must be combined with latitude)\n",
    "        api_key (str): YELP API to use for query\n",
    "        categories (str) : Foursquare-recognized place type. If not passed no place_type will be specified. Separate ids with commas\n",
    "    \n",
    "    Returns:\n",
    "        response: response object from the requests library.\n",
    "    \"\"\"  \n",
    "    url = f'https://api.yelp.com/v3/businesses/search?latitude={str(latitude)}&longitude={str(longitude)}&radius={str(radius)}&categories=&sort_by=best_match&limit=50'\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Authorization\": f'Bearer {api_key}'\n",
    "    }\n",
    "    response = requests.get(url=url,headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse through the response to get the POI (such as restaurants, bars, etc) details you want (ratings, name, location, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set my base variables for the search\n",
    "categories=\"None-Set\"\n",
    "#dropping to radius of 500 as I found maxed out my call for too many results\n",
    "radius = 500\n",
    "responses = []\n",
    "#run through the dataframe and search for all POI's in the radius of the longitude and latitude\n",
    "for index, row in bike_df.iterrows():\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "    res = get_venues_yelp(latitude, longitude, radius, YELP_KEY, categories)\n",
    "    # make sure we got a valid response\n",
    "    if res.status_code == 200:\n",
    "        responses.append(res.json())\n",
    "    else:\n",
    "        print(\"fail at: \", index, res.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the yelp dataframe and populate it filling in the bike station id for every call\n",
    "yelp_df = pd.DataFrame()\n",
    "i = 0\n",
    "while i < len(responses):\n",
    "    try:\n",
    "        temp_df = pd.DataFrame(responses[i]['businesses'])\n",
    "        temp_df['bs_id'] = bike_df.at[i, 'bs_id']\n",
    "        i += 1\n",
    "        yelp_df = pd.concat([yelp_df, temp_df], ignore_index = True)\n",
    "    except:\n",
    "        print(\"fail at: \", i)\n",
    "        i+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split out the columns that still have a series stored in them\n",
    "categories_series = yelp_df['categories']\n",
    "coordinates_series = yelp_df['coordinates']\n",
    "location_series = yelp_df['location']\n",
    "#don't currently need business hours, will come back to this if we change our mind\n",
    "business_hours_series = yelp_df['business_hours']\n",
    "\n",
    "# drop the columns we pulled out above\n",
    "yelp_df.drop(['location', 'business_hours', 'coordinates', 'categories'], axis = 1, inplace=True)\n",
    "\n",
    "#rename id's and distance column\n",
    "yelp_df.rename(columns={'id':'yelp_id', 'distance':'yelp_distance'}, inplace=True)\n",
    "\n",
    "\n",
    "#categories do not have ID's like with foursquare so the category column will be sets of the titles\n",
    "categories_list = []\n",
    "def extract_ids(data): #quick function to go through the json and return me all of the sets of the titles's (AI assisted for building this function)\n",
    "  ids = [item['title'] for item in data]\n",
    "  return set(ids)\n",
    "\n",
    "for item in categories_series:\n",
    "  id_set = extract_ids(item)\n",
    "  categories_list.append(id_set)\n",
    "yelp_df['categories'] = categories_list\n",
    "\n",
    "#lets seperate out the categories for a more normalised database when we get to it(in and out and in and out)\n",
    "yelp_catagories_df = yelp_df[['yelp_id','categories']].explode('categories')\n",
    "\n",
    "#lets put the coordinates back in\n",
    "coord_df = coordinates_series.apply(pd.Series)\n",
    "yelp_df = pd.merge(yelp_df, coord_df, left_index=True, right_index=True)\n",
    "\n",
    "#put the address back in:\n",
    "local_df = location_series.apply(pd.Series)\n",
    "local_df.drop('display_address',axis = 1, inplace = True)\n",
    "yelp_df = pd.merge(yelp_df, local_df, left_index=True, right_index=True)\n",
    "\n",
    "#get rid of all these columns that are unnesscary to the problems at hand and will just waste space\n",
    "yelp_df.drop(['transactions',\n",
    "               'phone',\n",
    "                 'display_phone',\n",
    "                   'address2',\n",
    "                     'address3',\n",
    "                       'city',\n",
    "                         'country',\n",
    "                           'state',\n",
    "                            'categories',\n",
    "                              'image_url',\n",
    "                                'url',\n",
    "                                  'is_closed',\n",
    "                                    'attributes',\n",
    "                                      'alias'], axis = 1, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframes to pickles\n",
    "yelp_df.to_pickle('../data/yelp_dataframe.pkl')\n",
    "yelp_catagories_df.to_pickle('../data/yelp_categories.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which API provided you with more complete data? Provide an explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Looking through the two API's I found more information, and more pertinant information in the yelp API.\n",
    " - The Yelp API includes ratings, if foursquare once did it does not seem to anymore.\n",
    " - Yelp API also includes a price rating so I can get an idea of which bike stations have more expensive things around them (categorical analysis coming your way?)\n",
    " - The one element I did prefer about Foursquare is they seperate \"Categories\" out with ID's which would allow me to normalize the database a little more, not so much with Yelp\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top 10 restaurants according to their rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[['name','rating']].sort_values('rating',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: left;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>name</th>\n",
    "      <th>rating</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>6522</th>\n",
    "      <td>Karnitas 71st</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>596</th>\n",
    "      <td>Paradise Cups</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>566</th>\n",
    "      <td>Karnitas 71st</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>586</th>\n",
    "      <td>Meat The Veggies</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>589</th>\n",
    "      <td>North Shore Historic District Marker</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>592</th>\n",
    "      <td>Vatito Taqueria</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>594</th>\n",
    "      <td>Makla Halal</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>595</th>\n",
    "      <td>The Scoop Coop</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>597</th>\n",
    "      <td>Schonberger Park</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>545</th>\n",
    "      <td>Berry Sweet Pavlovas</td>\n",
    "      <td>5.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
